%<?
% 
% $Star->addHook(qr/^%slide:(.*\n)/,sub{my $self=shift; my $all=shift;
%    my $arg=shift; return $arg; });
% $Star->addHook(qr/^%sl,l:(.*\n)/,sub{my $self=shift; my $all=shift;
%    my $arg=shift; return $arg; });
% $Star->addHook(qr/^[ \t]*\n/, sub { return "\n" });
% $Star->addHook(qr/^.*\n/, 'comment');
%!>

%sl,l:\newcommand{\lecturenumber}{4}
%sl,l:\newcommand{\lecturedate}{17-Sep-2007}

%slide:\input{slides-start}
%slide:\foilhead{}\begin{center}CSCI 6509\\Advanced Topics in Natural Language Processing\end{center}
%slide:
%slide:\vspace{1cm}
%slide:\begin{center}
%slide:\hrule
%slide:
%slide:\vspace{1cm}
%slide:%\begin{tabbing}
%slide:%xxxxxxxxxxxxxxxx\=\kill
%slide:%\>  Time:\' Tuesdays and Thursdays 13:05--14:25\nltabbing
%slide:%\> Location:\' Teaching Lab 3, CS building\nltabbing
%slide:%\end{tabbing}
%slide:
%slide:{\large\bf
%slide:%Part I: Linguistic Background\\[2ex]
%slide:Lecture \lecturenumber:\\[2ex]
%slide:Syntax; Semantics; Probabilistic Approach to NLP\\[2ex]
%slide:\rm\normalsize
%slide:\url{http://www.cs.dal.ca/~vlado/csci6509}\\[2ex]
%slide:Vlado Keselj\\[2ex]
%slide:Faculty of Computer Science\\[2ex]
%slide:Dalhousie University
%slide:}
%slide:\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Lecture Notes
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\lecture{\lecturedate}{Syntax; Semantics; Probabilistic Approach to NLP}

%\vspace{1cm}\hrule
\section*{Previous Lecture}

%slide:\foilhead{Previous Lecture}
%slide:\par\vspace{1cm}\hrule\vspace{1cm}

%sl,l:\begin{center}
%sl,l:\begin{itemize}
%sl,l: \item Closed word categories (continued): \begin{itemize}
%sl,l:  \item possesive pronouns (PRP\$)
%sl,l:  \item Wh-pronouns (WP) and Wh-possessives (WP\$)
%sl,l:  \item prepositions (IN)
%sl,l:  \item particles (RP)
%sl,l:  \item possesive ending (POS)
%sl,l:  \item modal verbs (MD), and auxiliaries
%sl,l:  \item infinitive word `to' (TO)
%sl,l:  \item qualifiers (RB)
%sl,l:  \item wh-adverbs (WRB)
%sl,l:  \item conjunctions (CC)
%sl,l:  \item interjections (UH)
%sl,l: \end{itemize}
%slide:\newpage
%sl,l: \item Open word categories
%sl,l: \begin{itemize}
%sl,l: \item Morphological processes:
%sl,l: \begin{itemize}
%sl,l: \item inflection \item derivation \item
%sl,l:        compounding \end{itemize}
%sl,l: \item nouns (NN, NNS, NNP, NNPS)
%sl,l: \item adjectives (JJ, JJR, JJS)
%sl,l: \item numbers (CD)
%sl,l: \item verbs (VB, VBP, VBZ, VBG, VBD, VBN)
%sl,l: \item adverbs (RB, RBR, RBS)
%sl,l: \end{itemize}
%sl,l:\end{itemize}
%sl,l:\end{center}

%\hrule\vspace{1cm}

\newpage
\subsection{Remaining POS Classes}

\subsubsection{Foreign Words (FW)}

Examples: de (tour de France),
perestroika, pro, des

\subsubsection{List Items (LS)}

Examples:
1, 2, 3, 4, a., b., c., first, second, etc.

\subsubsection{Punctuation}

\begin{tabbing}
Examplesxxxxxxxxx\=Tagxxxxx\=Description\kill
Examples\>Tag\>Description\\
\rule[2mm]{.6\textwidth}{0.5pt}\\
\verb/,/		\>\verb/,/	\>comma\\
\verb/; : ... - --/	\>\verb/:/	\>midsentence separator\\
\verb/. ! ?/		\>\verb/./	\>sentence end\\
\verb/( { [ </		\>\verb/(/	\>open parenthesis\\
\verb/) } ] >/		\>\verb/)/ 	\>closed parenthesis\\
\verb/` `` non-``/	\>\verb/``/	\>open quote\\
\verb/' ''/		\>\verb/''/	\>closed quote\\
\verb/$ c HK$ CAN$/	\>\verb/$/	\>dollar sign\\
\verb/#/		\>\verb/#/	\>pound sign\\
\verb/- + & @ * ** ffr/ \>\verb/SYM/	\>everything else
\end{tabbing}


%slide:\foilhead{Remaining POS Classes}
%slide:
%slide:\vspace{1cm}{\bf --- Foreign Words (FW)}
%slide:
%slide:Examples: de (tour de France),
%slide:perestroika, pro, des
%slide:
%slide:\vspace{1cm}{\bf --- List Items (LS)}
%slide:
%slide:Examples:
%slide:1, 2, 3, 4, a., b., c., first, second, etc.
%slide:
%slide:\vspace{1cm}{\bf --- Punctuation}
%slide:
%slide:\foilhead{Punctuation}
%slide:
%slide:\begin{tabbing}
%slide:Examplesxxxxxxxxxxxxxxxxx\=Tagxxxxx\=Description\kill
%slide:Examples\>Tag\>Description\\
%slide:\rule[2mm]{.8\textwidth}{0.5pt}\\
%slide:\verb/,/		\>\verb/,/	\>comma\\
%slide:\verb/; : ... - --/	\>\verb/:/	\>midsentence separator\\
%slide:\verb/. ! ?/		\>\verb/./	\>sentence end\\
%slide:\verb/( { [ </		\>\verb/(/	\>open parenthesis\\
%slide:\verb/) } ] >/		\>\verb/)/ 	\>closed parenthesis\\
%slide:\verb/` `` non-``/	\>\verb/``/	\>open quote\\
%slide:\verb/' ''/		\>\verb/''/	\>closed quote\\
%slide:\verb/$ c HK$ CAN$/	\>\verb/$/	\>dollar sign\\
%slide:\verb/#/		\>\verb/#/	\>pound sign\\
%slide:\verb/- + & @ * ** ffr/ \>\verb/SYM/	\>everything else
%slide:\end{tabbing}

%slide:\foilhead{Some Tagged Examples}
%slide:\begin{verbatim}
%slide:The/DT grand/JJ jury/NN commented/VBD on/IN
%slide:a/DT number/NN of/IN other/JJ topics/NNS ./.
%slide:
%slide:Book/VB that/DT flight/NN ./.
%slide:
%slide:Does/VBZ that/DT flight/NN serve/VB dinner/NN ?/.
%slide:
%slide:It/PRP does/VBZ a/DT first-rate/JJ job/NN ./.
%slide:
%slide:``/`` When/WRB the/DT sell/NN programs/NNS hit/VBP
%slide:,/, you/PRP can/MD hear/VB the/DT order/NN
%slide:printers/NNS start/VB to/TO go/VB ''/'' on/IN the/DT
%slide:Big/NNP Board/NNP trading/NN floor/NN ,/, says/VBZ
%slide:one/CD specialist/NN there/RB ./.
%slide:
%slide:``/`` Do/VBP you/PRP make/VB sweatshirts/NNS or/CC
%slide:sparkplugs/NNS ?/.
%slide:\end{verbatim}


\clearpageforcoursenotes

%slide:\foilhead{Syntax}
%slide:
%slide:Reading: Chapter 9
%slide:
%slide:Syntax $=$ sentence structure; i.e., study of the phrase structure
%slide:
%slide:\begin{itemize}
%slide:\item words are not randomly ordered ---
%slide:word order is important and non-trivial
%slide:\item There are ``free-order'' langauges (e.g., Latin, Russian), but
%slide:  they are not completely free-order.
%slide:\item a hierarchical view of sentence structure: \begin{itemize}
%slide:\item words form phrases
%slide:\item phrases form clauses
%slide:\item clauses form sentences \end{itemize}
%slide:\end{itemize}
%slide:
%slide:The main NLP problem in syntax is {\em parsing;} i.e., given a
%slide:sentence, find a correct structure of the sentence (typically a parse
%slide:tree).

\section{Syntax}

Syntax $=$ Phrase Structure

\begin{itemize}
\item words are not randomly ordered\\
word order is important and non-trivial
\item There are ``free-order'' langauges (Latin Russian)\\
not completely free-order
\item words form phrases
\item phrases form clauses
\item clauses form sentences
\end{itemize}

The main NLP problem in syntax is {\em parsing;} i.e., given a
sentence, find a correct structure of the sentence

``Structure'' is typically a parse tree.

%slide:\foilhead{Example}
%slide:
%slide:That man caught the butterfly with a net.
%slide:\begin{itemize}
%slide:\item Syntactic tree example 
%slide:\item Induced grammar example
%slide:\item Context-Free Grammar example
%slide:\end{itemize}

\subsection*{Example}

That man caught the butterfly with a net.

\subsection{Context-Free Grammars (CFG)}
%slide:\foilhead{Context-Free Grammars (CFG)}
%slide:
%slide:CFG is a touple $(V,T,P,S)$
%slide:
%slide:\begin{itemize}
%slide:\item $V$ is a set of variables or non-terminals, e.g., $V=\{$S, NP, DT, NN, VP, VBD, PP, IN$\}$
%slide:\item $T$ is a set of terminals, words, or lexemes, e.g., $T=\{$That,
%slide:man, caught, the, butterfly, with, a, net$\}$
%slide:\item $S$ is the start symbol $S\in T$
%slide:\item $P$ is a set of rules in the form:
%slide:\[ \textit{S} \rightarrow \textit{NP}\;\textit{VP}, \;\; \textit{NP}
%slide:\rightarrow \textit{DT}\;\textit{NN}, \;\;
%slide:\textit{DT}\rightarrow \textrm{That} \ldots \]
%slide:\end{itemize}

CFG is a touple $(V,T,P,S)$

\begin{itemize}
\item $V$ is a set of variables or non-terminals, e.g., $V=\{$S, NP, DT, NN, VP, VBD, PP, IN$\}$
\item $T$ is a set of terminals, words, or lexemes, e.g., $T=\{$That,
man, caught, the, butterfly, with, a, net$\}$
\item $S$ is the start symbol $S\in T$
\item $P$ is a set of rules in the form:
\begin{verbatim}
S -> NP VP, NP -> DT NN, DT -> That, etc.
\end{verbatim}
\end{itemize}

%slide:\foilhead{Some Notions Related to CFGs}


\subsection{Bracket Representation of a Parse Tree}

\begin{verbatim}
(S (NP (DT That)
       (NN man))
   (VP (VBD caught)
       (NP (DT the)
           (NN butterfly))
       (PP (IN with)
           (NP (DT a)
               (NN net)
)  )   )   )
\end{verbatim}

%slide:\foilhead{Bracket Representation of a Parse Tree}
%slide:
%slide:\vspace{2cm}
%slide:\begin{verbatim}
%slide:(S (NP (DT That)
%slide:       (NN man))
%slide:   (VP (VBD caught)
%slide:       (NP (DT the)
%slide:           (NN butterfly))
%slide:       (PP (IN with)
%slide:           (NP (DT a)
%slide:               (NN net)
%slide:)  )   )   )
%slide:\end{verbatim}

\subsection{Typical Natural Language CF Rules}

\subsubsection{Sentence (S)}

\begin{verbatim}
S -> NP VP
S -> VP
S -> Aux NP VP
S -> Wh-NP VP
S -> Wh-NP Aux NP VP
S -> NP
\end{verbatim}

%slide:\foilhead{Typical Phrase Structure in English}
%slide:
%slide:\mytitle{Sentence (S)}
%slide:
%slide:\begin{verbatim}
%slide:S -> NP VP
%slide:S -> VP
%slide:S -> Aux NP VP
%slide:S -> Wh-NP VP
%slide:S -> Wh-NP Aux NP VP
%slide:S -> NP
%slide:\end{verbatim}

\subsubsection{Noun Phrase (NP)}

Typical:
\begin{verbatim}
NP -> DT JJ* NN PP*
\end{verbatim}

%slide:\mytitle{Noun Phrase (NP)}
%slide:
%slide:Typical:
%slide:\begin{verbatim}
%slide:NP -> DT JJ* NN PP*
%slide:\end{verbatim}

\subsubsection{Prepositional Phrase (PP)}

Typical:
\begin{verbatim}
PP -> IN NP
\end{verbatim}

\subsubsection{Verb Phrase (VP)}

Typical:
\begin{verbatim}
VP -> VB PP*
VP -> VB NP PP*
VP -> VB NP NP PP*
\end{verbatim}

%slide:\foilhead{Prepositional Phrase (PP)}
%slide:
%slide:Typical:
%slide:\begin{verbatim}
%slide:PP -> IN NP
%slide:\end{verbatim}
%slide:
%slide:\mytitle{Verb Phrase (VP)}
%slide:
%slide:Typical:
%slide:\begin{verbatim}
%slide:VP -> VB PP*
%slide:VP -> VB NP PP*
%slide:VP -> VB NP NP PP*
%slide:\end{verbatim}
%slide:
%slide:\mytitle{Adjective Phrase (AP)}
%slide:
%slide:\begin{itemize}
%slide:\item less common
%slide:\item example: She is {\em very sure of herself.}
%slide:\end{itemize}
%slide:
%slide:\foilhead{Are Natural Languages Context-Free?}
%slide:
%slide:CFGs are usually not sufficient for NL parsing because of Natural
%slide:Language Phenomena.

\subsubsection{Adjective Phrase (AP)}

\begin{itemize}
\item less common
\item example: She is {\em very sure of herself.}
\end{itemize}


\bigskip
CFGs are usually not sufficient for NL parsing because of Natural
Language Phenomena.

\subsection{Natural Language Phenomena}

Examples: Agreement, Movement, Subcategorization

\subsubsection{Agreement}

\begin{itemize}
\item subject-verb agreement

For example, ``I work.'' and ``He works.'' vs. *``I works.'' and *``He work.''

\item specifier-head agreement

For example, ``This book.'' and ``These books.'' vs. *``This books.''
and ``These book.''

\end{itemize}

Agreement can be a non-local dependency, e.g:

The {\bf women} who found the wallet {\bf were} given a reward.
\vspace{1cm}

\subsubsection{Movement}

E.g, wh-movement

\begin{tabbing}
{\bf Which book} should Peter buy \= ?\\
{\em filler} \> {\em gap}
\end{tabbing}


\subsubsection{Subcategorization}

Example:\\
The problem disappeared. and\\
The defendant denied the accusation.
\\are two valid sentences, however, the following two are
grammatically incorrect:\\
*The problem disappeared the accusation. and\\
*The defendant denied.

Explanation:
\begin{itemize}
\item ``disappear'' does not take an object (verb valence)
\item ``deny'' requires an object
\end{itemize}


\subsection{Heads and Dependency}

\subsubsection{Heads and Dependency}

\begin{itemize}
\item the parse tree of ``That man caught the butterfly with a net.''
\item annotate dependencies, head words
\end{itemize}

\centerline{\includegraphics[height=5cm]{heads}}

\subsubsection{Head-feature Principle}

The features of a phrase are normally transfered from the features of
the head word.

\subsubsection{Dependency Tree}

\begin{itemize}
\item dependency grammar
\item example with ``That man caught the butterfly with a net.''
\end{itemize}

\section{Semantics}

\subsection{Lexical Semantics}

\begin{itemize}
\item WordNet
\item some relations in lexical semantics:
\begin{itemize}
	\item synonyms, synsets
	\item hypernyms
	\item hyponyms
	\item antonyms
	\item meronyms
	\item holonyms
	\item homonyms
	\item polisemy
\end{itemize}
\end{itemize}


\subsection{Semantic Compositionality}

How meaningins of the pieces combine into a meaning of the whole?

Levels of compositionality:

\begin{enumerate}
\item compositional semantics

e.g., white paper $=$ white $+$ paper

\item collocations

e.g., while wine $\approx$ while $+$ wine

\item idioms

e.g., kick the bucket $\not=$ kick $+$ the bucket
\end{enumerate}

\subsection{Semantic Roles}

Syntax is closely relation to semantics.

For example, subcategorization frames can be used to assign
\terms{semantic roles} of the verb aguments.

E.g., verb {\em send,} semantic frame: NP[subject], NP[indirect
object] NP[direct object]\\
The elements of the frame may correspond to semantic roles of: SENDER,
RECIPIENT, and OBJECT, resulting in the frame:
\[ \avm{\type{send} \\ \attval{SENDER}{I} \\
     \attval{RECIPIENT}{you}\\
     \attval{OBJECT}{an e-mail}}
\]

Semantic preference can be used to properly disambiguate the
sentences:\\
He ate the cake with a frosting.\ \ \ and\\
He ate the cake with a spoon.

\vspace{1cm}
\hrule
\vspace{1cm}

Relevant reading in the textbook regarding some previous material:
\begin{itemize}
\item in chapter 9: 9.1--8, syntax
\item in chapter 11: 11.3, agreement, head features,
      subcategorization, dependencies (but we will revisit it), 
\item in chapter 12: 12.4, dependency structure
\item in chapter 14: 14.1--2 represnting meaning, predicate-argument structure
\item in chapter 15: 15.4, idioms and compositionality
\item in chapter 16: 16.1--3, lexical semantics
\end{itemize}

\vspace{1cm}\hrule

\newpage
\part{Statistical Approach to NLP}

\section{Logical versus Plausible Reasoning}

Two kinds of reasoning (i.e. inference) in AI:
\begin{enumerate}
\item logical reasoning (classical, knowledge-based AI)
\item plausible reasoning (uncertain, non-monotonic)
\end{enumerate}

\paragraph{Task of Plausible or Uncertain Reasoning:}\ \\
Plausible inference of a hidden structure from observations (inputs).

\subsubsection*{Examples:}
\begin{tabular}{ccc}
Input & & Hidden Structure\\ \hline
word sequence & $\rightarrow$ & meaning \\
pixel matrix  & $\rightarrow$ & object, relations \\
speech signal & $\rightarrow$ & phonemes, words\\
words in e-mail Subject: & $\rightarrow$ & Is message spam? Yes/No \\
symptoms                 & $\rightarrow$ & illness \\ \hline
\end{tabular}

\bigskip
How to combine ambiguous, incomplete, contradicting evidence, and
draw reasonable conclusions?

\subsubsection*{Logical vs Plausible reasoning}

\begin{itemize}
\item plausible inference:
	\begin{itemize}
	\item non-monotonic: might change conclusions given more
		evidence
	\item uncertain: conclusions not guaranteed to be correct (but
		still want to do as well as possible)
	\end{itemize}
\item logical inference:
	\begin{itemize}
	\item monotonic: once conclusion drawn never retracted
	\item certain: conclusions certain given assumptions
	\end{itemize}
\end{itemize}

Two approaches to NLP: 
\begin{itemize}
\item Logical (symbolic, knowledge-based) and
\item Plausible (uncertain, usually probabilistic, i.e., stochastic)
\end{itemize}

\bigskip
{\bf Theory of Probability} is not the only plausible-inference approach to
NLP.  Alternatives:
\begin{itemize}
\item neural networks
\item fuzzy logic, fuzzy sets
\item default logic
\item rule-based systems
\item Dempster-Shafer theory
\item rough sets,
 \ldots
\end{itemize}

\section{Counting Words and N-grams}

\begin{itemize}
\item counting letters
\item counting words, e.g., ``Tom Sawyer'':

\begin{tabular}{ccc}
Word & Freq ($f$) & Rank ($r$) \\ \hline
the    & 3332 & 1 \\
and    & 2972 & 2 \\
a      & 1775  &3   \\
to     & 1725  & 4\\
of     & 1440  & 5\\
was    & 1161  & 6\\
it     & 1027  & 7\\
in     &  906  & 8\\
that   &  877  & 9\\
he     &  877  & 10\\
I      &  783  & 11\\
his    &  772  & 12\\
you    &  686  & 13\\
Tom    &  679  & 14\\
with   &  642  & 15\\
\vdots & \vdots
\end{tabular}

\item Zipf's law (1929): $r \times f \approx \mbox{const.}$

\item counting n-grams

\item application to text classification
\end{itemize}

\subsection{Text Classification}

or Text Categorization

The Problem of Classification

Evaluation Measures

Issues: overfitting and underfitting

Evaluation Methods:
\\1. training error
\\2. train and test
\\3. n-fold cross-validation

Vector space model

\[\var{tfidf} = \var{tf} \cdot \log\left(\frac{N}{\var{df}}\right)\]

Some of the well-known methods: Na\"{\i}ve Bayes, kNN

kNN method

% Lecture 05
%\lecture{21-Sep-2006}{Character N-gram-based Text Classification}

\newcommand{\itm}[1]{\par\vspace{1cm}{\large #1}}

\itm{Supervised vs. Unsupervised Learning}

\itm{Clustering and Classification}

\itm{Clustering (k-means method)}

\subsection{Types of Text Classification}

Some types of text classification:

\begin{itemize}
\item spam detection and e-mail classification
\item text coding
\item encoding and langauge identification
\item sentiment classification
\item authorship attribution and plagiarism detection
\item automatic essay grading
\item topic categorization
\end{itemize}

More specialized: dementia detection using spontaneus speech

\subsection{CNG---Common N-Gram analysis for text classification}

\itm{Application to authorship attribution}

\begin{itemize}
\item creating n-gram based author profiles 
\item similarity measure
\item kNN method
\end{itemize}


\section{Elements of Probability Theory}
\begin{itemize}
\item simple event

Examples: rolling a dice, choosing a letter

P('a') = ?; probability of choosing a letter; $1/26\approx 0.04$; it
is 0.08

\itm{}
\item independent events (definition)

Example: choosing two letters vs. choosing two consecutive letters

choosing t: 0.1, h:0.07; choosing 't' and 'h' independently: 0.007;
consecutive 'th' = 0.04; not independent events

\itm{}
\item random variables

{\em Independent random variables} $V_1$ and $V_2$:
\[ \PP(V_1=x_1, V_2=x_2) = \PP(V_1=x_1) \cdot \PP(V_2=x_2) \]

\item conditional probability

{\em Conditional probability:} example, 
\[ \PP(A|B) = \frac{\PP(A,B)}{\PP(B)} \]

\item expressing independency using conditional probability

Alternative definition of independent variables (proof):
\[ \PP(V_1=x_1| V_2=x_2) = \PP(V_1=x_1) \]

\item Bayes' theorem

{\em Bayes' theorem} (one form) and proof:
\[ \PP(A|B) = \frac{\PP(B|A)\cdot\PP(A)}{\PP(B)} \]

\item {\em Conditionally independent variables}

Random variables $V_1$ and $V_2$ are {\em conditionally independent given\/} 
$V_3$ if
$\PP(V_1\!=\!x_1,V_2\!=\!x_2|V_3\!=\!x_3)=\PP(V_1\!=\!x_1|V_3\!=\!x_3)\PP(V_2\!=\!x_2|V_3\!=\!x_3)$
for all $x_1,x_2,x_3$.
Equivalently, if
$\PP(V_1\!=\!x_1|V_2\!=\!x_2,V_3\!=\!x_3)=\PP(V_1\!=\!x_1|V_3\!=\!x_3)$ for all $x_1,x_2,x_3$.

\end{itemize}

\itm{Generative models}

\begin{itemize}
\item truth $\rightarrow$ evidence; speech recognition,
explanation of `arg max'
\end{itemize}

% Lecture 06
%\lecture{26-Sep-2006}{Probabilistic Modeling}

\subsection{Generative Models}

\begin{enumerate}
\item represent knowledge with probabilistic models: forward generative
 models:
\[\begin{array}{ccc}
\downarrow & \quad & \textrm{world}\\
\framebox{truth} & & \downarrow\\
\downarrow && \textrm{sensor}\\
\framebox{evidence} && \downarrow\\
&& \textrm{observed measurement}
  \end{array}
\]

\item principle of evidence combination: Bayesian inference
\begin{eqnarray*}
\textrm{conclusion} &=& \argmax_{\textrm{\scriptsize possible truth}}
P(\textrm{possible truth}|\textrm{evidence})\\
&=& \argmax_{\textrm{\scriptsize possible truth}} 
	{   P(\textrm{evidence} | \textrm{possible truth})
		P(\textrm{possible truth}) \over
  		P(\textrm{evidence}) }\\
&=& \argmax_{\textrm{\scriptsize possible truth}}
           P(\textrm{evidence} | \textrm{possible truth})
                P(\textrm{possible truth})
\end{eqnarray*}
\end{enumerate}

\begin{itemize}
\item application to speech recognition: acoustic model and langauge model
\end{itemize}

\section{Probabilistic Modeling}

\subsubsection*{Random variables}

We assume that we have a set of $n$ random variables describing our problem:
\[
\VV = (V_1, V_2,..., V_n)
\]
Each variable can be assigned a value from a finite set of different
values.  These values are usually denoted as $\{x_1, x_2, \ldots, x_m\}$.

\subsubsection*{Random configuration}

A tuple of values, i.e., a vector $\xvec=(x_1, x_2, \ldots, x_n)$,
where each value is assigned to a variable: $V_1=x_1$, $V_2=x_2$,
\ldots is called a \terms{random configuration.}

In modeling our problem we assume that a sequence of configurations
$\xvec^{(1)},...,\xvec^{(t)}$ is drawn from some random source:
\[
\begin{array}{cccccc}
\xvec^{(1)} & = & (x_{11} & x_{12} & \dots & x_{1n})\\
\xvec^{(2)} & = & (x_{21} & x_{22} & \dots & x_{2n})\\
& \vdots\\
\xvec^{(t)} & = & (x_{t1} & x_{t2} & \dots & x_{tn})\\
\end{array}
\]
Again, we assume a fixed number $n$ of components in each configuration,
and assume values $x_{ij}$ are from a finite set $\{x_1, x_2, \ldots, x_m\}$.

\subsection{Computational Tasks in Probabilistic Modeling}

\begin{description}
\item[1. \em Simulation:] generate random configurations
\item[2. \em Evaluation:] compute probability of a complete configuration
\item[3. \em Inference] has the following sub-tasks:
	\begin{description}
	\item[3.a \em Marginalization:] computing probability of a partial
	configuration,
	\item[3.b \em Conditioning:] computing conditional probability of a
	completion given an observation,
	\item[3.c \em Completion:] finding the most probable
	completion, given an observation
	\end{description}
\item[4. \em Learning:] learning parameters of a model.
\end{description}

Let us use an example to illustrate this:

\subsubsection*{Example: Spam Detection}

The problem of spam detection in e-mail is the problem of
automatically detecting whether an arbitrary e-mail message is spam or
not.
In a toy model, we assume that we can detect whether a message is spam
or not relying only on the fact whether the `Subject:' header of the
message is capitalized (i.e., completely written in uppercase letters) and
whether the `Subject:' header contains the word `free' (uppercase or
lowercase).
For example, ``NEW MORTGAGE RATE'' is likely the subject of a spam
message, as well as ``Money for Free,'' ``FREE lunch,'' etc.

Hence, our model is based on the following three random variables and
each of them gets one of two values Y (for Yes) or N (for No):
\begin{eqnarray*}
\mbox{\em Caps} &=& \parbox[t]{.8\textwidth}{Y if and
only if the Subject of the message does not contain lowercase letters,}\\
\mbox{\em Free} &=& \parbox[t]{.8\textwidth}{Y if and only if the word
`free' appears in the Subject (letter case is ignored), and}\\
\mbox{\em Spam} &=& \parbox[t]{.8\textwidth}{Y if and only if the
message is spam.}
\end{eqnarray*}

In order to learn what happens in real-world, we open into our
mailbox, which serves as our random source, randomly select 100
messages and count how many times each configuration appears.

We might obtain the following table:

\begin{center}
\begin{tabular}{|c|c|c|c|}
{\em Free} & {\em Caps} & {\em Spam} & Number of messages\\
\hline
Y & Y & Y & 20  \\
Y & Y & N & \ 1 \\
Y & N & Y & \ 5 \\
Y & N & N & \ 0 \\
N & Y & Y & 20  \\
N & Y & N & \ 3 \\
N & N & Y & \ 2 \\
N & N & N & 49  \\
\hline
\multicolumn{3}{|r|}{Total:} & 100 \\
\hline
\end{tabular}
\end{center}

Let us consider our first, straightforward model, called \terms{Joint
Distribution Model:}

\subsection{Joint Distribution Model}

In the Joint distribution model, we specify the complete \terms{joint
probability distribution,} i.e., the probability 
$\PP(V_1\!=\!x_1,...,V_n\!=\!x_n)$
for each complete configuration $\xvec=(x_1,...,x_n)$.
In general, we need $m^n$ parameters (minus one constraint) to specify 
an arbitrary joint distribution on $n$ random variables with $m$ values.
One could represent this by a lookup table
%$\theta_{\xvec^{(1)}},\theta_{\xvec^{(2)}},...,\theta_{\xvec^{(V^n)}}$,
$p_{\xvec^{(1)}},p_{\xvec^{(2)}},\ldots,p_{\xvec^{(m^n)}}$,
where 
$p_{\xvec^{(\ell)}}$ 
gives the probability that the random variables jointly take on
configuration $\xvec^{(\ell)}$;
that is, 
%$\theta_{\xvec^{(\ell)}}=\PP(\XX\!=\!\xvec^{(\ell)})$.
$p_{\xvec^{(\ell)}}=\PP(\VV\!=\!\xvec^{(\ell)})$.
These numbers are positive and satisfy the constraint that
%$\sum_{\ell=1}^{V^n}\theta_{\xvec^{(\ell)}}=1$.
$\sum_{\ell=1}^{m^n}p_{\xvec^{(\ell)}}=1$.

\subsubsection*{Example: Spam Detection (continued)}
To estimate the joint distribution in our spam detection example, we
can simply divide the number of message for each configuration with
the total number of messages:

\begin{center}
\begin{tabular}{|c|c|c|c||c|}
{\em Free} & {\em Caps} & {\em Spam} & Number of messages & $\PP$\\
\hline
Y & Y & Y & 20  & 0.20\\
Y & Y & N & \ 1 & 0.01\\
Y & N & Y & \ 5 & 0.05\\
Y & N & N & \ 0 & 0.00\\
N & Y & Y & 20  & 0.20\\
N & Y & N & \ 3 & 0.03\\
N & N & Y & \ 2 & 0.02\\
N & N & N & 49  & 0.49\\
\hline
\multicolumn{3}{|r|}{Total:} & 100 & 1.00\\
\hline
\end{tabular}
\end{center}

Estimating probabilities in this way is known as {\em Maximum
Likelihood Estimation\/} (MLE), since it can be shown that in this way
the probability $\PP(T|M)$, where $T$ is our training data and $M$ is
the model, is maximized in terms of~$M$.

Using this model, we can estimate the probability of any
configuration, e.g.:
\[\PP(\mbox{\em Free}=Y,\mbox{\em Caps}=N,\mbox{\em Spam}=N) =
  0.00 \]
This example illustrates a drawback of the full joint distribution
model: the \terms{sparse data problem.}
We concluded that the probability of this configuration is~0, i.e.,
that it is impossible, based on the fact that we have not seen it
before.
However, not seeing a configuration does not necessarily mean that it
cannot appear in the future.


\subsubsection*{1. Simulation}

%\begin{itemize}
%\item[{\bf S1}.] {\em Sample}.
Draw a complete configuration $\xvec$ according to the joint distribution.
Given the lookup table representation, one could just compute the cumulative
value of the
%$\theta_{\xvec^{(\ell)}}$'s, draw a random number $p$ 
$p_{\xvec^{(\ell)}}$'s, draw a random number $p$ 
between 0 and 1,
and select the configuration $\xvec^{(\ell)}$ whose cumulative probability 
interval contains $p$.
%\end{itemize}

\subsubsection*{2. Evaluation}

%\begin{itemize}
%\item[{\bf E1}.] {\em Evaluate}.
Evaluate the probability of a complete configuration $\xvec=(x_1,...,x_n)$.
In the table lookup representation, one can just look up the answer:
\[
%\PP(X_1\!=\!x_1,...,X_n\!=\!x_n)=\theta_{(x_1,x_2,...,x_n)}
\PP(V_1\!=\!x_1,...,V_n\!=\!x_n)=p_{(x_1,x_2,...,x_n)}
\]
%\end{itemize}

\subsubsection*{3. Inference}

\begin{Itemize}
\item[3.a] {\em Marginalization}.
Compute the probability of an {\em incomplete\/} configuration
$\PP(X_1\!=\!x_1,...,X_k\!=\!x_k)$, where $k<n$:
%
\begin{eqnarray*}
%\lefteqn{\PP(X_1\!=\!x_1,...,X_k\!=\!x_k)}
\lefteqn{\PP(V_1\!=\!x_1,...,V_k\!=\!x_k)}
\\
& = &
\sum_{y_{k+1}}\cdots\sum_{y_n}
\PP(V_1\!=\!x_1,...,V_k\!=\!x_k,V_{k+1}\!=\!y_{k+1},...,V_n\!=\!y_n)
%\PP(X_1\!=\!x_1,...,X_k\!=\!x_k,X_{k+1}\!=\!y_{k+1},...,X_n\!=\!y_n)
\\
& = &
%\sum_{y_{k+1}}\cdots\sum_{y_n} \theta_{(x_1,...,x_k,y_{k+1},...,y_n)}
\sum_{y_{k+1}}\cdots\sum_{y_n} p_{(x_1,...,x_k,y_{k+1},...,y_n)}
\end{eqnarray*}
%
We need to be able to evaluate complete configurations and then sum
over $m^{n-k}$ possible completions, where $m$ is the number of
elements in the domain of $y_{k+1}, \ldots, y_n$.

\item[3.b] {\em Conditioning}.
Compute the conditional probability of a possible completion 
$(y_{k+1},...,y_n)$ given an incomplete configuration $(x_1,...,x_k)$.
%
\begin{eqnarray*}
%\lefteqn{\PP(X_{k+1}\!=\!y_{k+1},...,X_n\!=\!y_n|X_1\!=\!x_1,...,X_k\!=\!x_k)}
\lefteqn{\PP(V_{k+1}\!=\!y_{k+1},...,V_n\!=\!y_n|V_1\!=\!x_1,...,V_k\!=\!x_k)}
\\
& = & 
%\frac{\PP(X_1\!=\!x_1,...,X_k\!=\!x_k,X_{k+1}\!=\!y_{k+1},...,X_n\!=\!y_n)}%
%{\PP(X_1\!=\!x_1,...,X_k\!=\!x_k)}
\frac{\PP(V_1\!=\!x_1,...,V_k\!=\!x_k,V_{k+1}\!=\!y_{k+1},...,V_n\!=\!y_n)}%
{\PP(V_1\!=\!x_1,...,V_k\!=\!x_k)}
\\
& = &
%\frac{\theta_{(x_1,...,x_k,y_{k+1},...,y_n)}}%
%{\sum_{z_{k+1}}\cdots\sum_{z_n}\theta_{(x_1,...,x_k,z_{k+1},...,z_n)}}
\frac{p_{(x_1,...,x_k,y_{k+1},...,y_n)}}%
{\sum_{z_{k+1}}\cdots\sum_{z_n}p_{(x_1,...,x_k,z_{k+1},...,z_n)}}
\end{eqnarray*}
%
Need to evaluate a complete configuration and then divide by a marginal sum.

\item[{\bf I3}.] {\em Completion}.
Find the most probable completion
$(y_{k+1}^*,...,y_n^*)$ given an incomplete configuration $(x_1,...,x_k)$.
%
\begin{eqnarray*}
y_{k+1}^*,...,y_n^*
& = & 
\argmax_{y_{k+1},...,y_n}
%\PP(X_{k+1}\!=\!y_{k+1},...,X_n\!=\!y_n|X_1\!=\!x_1,...,X_k\!=\!x_k)
\PP(V_{k+1}\!=\!y_{k+1},...,V_n\!=\!y_n|V_1\!=\!x_1,...,V_k\!=\!x_k)
\\
& = &
\argmax_{y_{k+1},...,y_n}
%\frac{\PP(X_1\!=\!x_1,...,X_k\!=\!x_k,X_{k+1}\!=\!y_{k+1},...,X_n\!=\!y_n)}%
%{\PP(X_1\!=\!x_1,...,X_k\!=\!x_k)}
\frac{\PP(V_1\!=\!x_1,...,V_k\!=\!x_k,V_{k+1}\!=\!y_{k+1},...,V_n\!=\!y_n)}%
{\PP(V_1\!=\!x_1,...,V_k\!=\!x_k)}
\\
& = & 
\argmax_{y_{k+1},...,y_n}
%\PP(X_1\!=\!x_1,...,X_k\!=\!x_k,X_{k+1}\!=\!y_{k+1},...,X_n\!=\!y_n)
\PP(V_1\!=\!x_1,\ldots,V_k\!=\!x_k,V_{k+1}\!=\!y_{k+1},...,V_n\!=\!y_n)
\\
& = &
\argmax_{y_{k+1},...,y_n}
%\theta_{(x_1,...,x_k,y_{k+1},...,y_n)}
p_{(x_1,...,x_k,y_{k+1},...,y_n)}
\end{eqnarray*}
%
Have to search through all $m^{n-k}$ possible completions
and evaluate each complete configuration to find the maximum.

\end{Itemize}

\subsubsection*{Learning}

Estimate probabilities by counting, i.e., using
\terms{maximum likelihood estimation.}

\paragraph{Drawbacks of the joint distribution model} are:
\begin{itemize}
\item memory cost to store table,
\item running-time cost to do summations, and
\item the sparse data problem in learning (i.e., training).
\end{itemize}

\paragraph{Other probability models} are ways of specifying specialized
joint distributions, in which we address these problems.

The goal is to impose structure on joint distribution
$\PP(V_1\!=\!x_1,...,V_n\!=\!x_n)$.
One key tool for imposing structure %on a joint distribution
is variable independence.

\subsection{Fully Independent Model}

In a fully independent model we assume that all variables are
independent, i.e., 
\[\PP(V_1\!=\!x_1,...,V_n\!=\!x_n) 
= \PP(V_1\!=\!x_1)\cdots\PP(V_n\!=\!x_n).\]
This yields a very restricted form of joint distribution
where we can represent each component distribution separately.
For a random variable $V_j$, one can represent $\PP(V_j\!=\!x)$ by a lookup 
table with $m$ parameters (minus one constraint).
Let $p_{j,x}$ denote the probability $V_j$ takes on value $x$.
That is, $p_{j,x}=\PP(V_j\!=\!x)$.
These numbers are positive and satisfy the constraint
$\sum_{x=1}^m p_{j,x}=1$ for each $j$.
Thus, the joint distribution over $V_1,...,V_n$ can
be represented by $n\times m$ positive numbers minus $n$ constraints.
The previous tasks (simulation, evaluation, and inference) % and learning
now become almost trivial.
Admittedly this is a silly model as far as real applications go,
but it clearly demonstrates the benefits of structure
(in its most extreme form).


\subsubsection*{Example: Spam Detection (continued)}

The fully independent model is almost useless in our spam detection
example because it assumes that the three random variables: {\em Caps,
Free,} and {\em Spam} are independent.
In other words, its assumption is that knowing whether a
message has a capitalized subject or contains the word `Free' in the
subject cannot help us in determining whether the message is spam or
not, which is not in accordance with our earlier assumption.

Anyway, let us see what happens when we apply the fully independent
model to our example.
From the training data:
\begin{center}
\begin{tabular}{|c|c|c|c||c|}
{\em Free} & {\em Caps} & {\em Spam} & Number of messages \\
\hline
Y & Y & Y & 20  \\
Y & Y & N & \ 1 \\
Y & N & Y & \ 5 \\
Y & N & N & \ 0 \\
N & Y & Y & 20  \\
N & Y & N & \ 3 \\
N & N & Y & \ 2 \\
N & N & N & 49  \\
\hline
\multicolumn{3}{|r|}{Total:} & 100 \\
\hline
\end{tabular}
\end{center}

we generate the following probability tables of independent variables:

{
\renewcommand{\arraystretch}{1.2}

\begin{tabular}{c|l}
{\em Free} & $\PP(\mbox{\em Free})$\\\hline
Y & $\frac{20+1+5+0}{100}=0.26$\\
N & $\frac{20+3+2+49}{100}=0.74$\\\hline
\end{tabular}
and similarly,

\begin{tabular}{c|l}
{\em Caps} & $\PP(\mbox{\em Caps})$\\\hline
Y & $\frac{20+1+20+3}{100}=0.44$\\
N & $\frac{5+0+2+49}{100}=0.56$\\\hline
\end{tabular}
and
\begin{tabular}{c|l}
{\em Spam} & $\PP(\mbox{\em Spam})$\\\hline
Y & $\frac{20+5+20+2}{100}=0.47$\\
N & $\frac{1+0+3+49}{100}=0.53$\\\hline
\end{tabular}
}

\bigskip
Hence, in this model any message is a spam with probability 0.47, no
matter what the values of {\em Caps} and {\em Free} are.

For example, the probability of configuration $(\mbox{\em Caps}=Y,\mbox{\em
Free}=N,\mbox{\em Spam}=N)$ in this model is:
\begin{eqnarray*}
\lefteqn{\PP(\mbox{\em Free}=Y,\mbox{\em Caps}=N,\mbox{\em Spam}=N) =}\\
&=&  \PP(\mbox{\em Free}=Y)\cdot
  \PP(\mbox{\em Caps}=N)\cdot
  \PP(\mbox{\em Spam}=N) = 0.26 \cdot 0.56 \cdot 0.53\\
&=& 0.077168 \approx 0.08
\end{eqnarray*}


\subsubsection*{1. Simulation}

%\begin{itemize}
%\item[{\bf S1}.] {\em Sample}.
For $j=1,...,n$, independently draw $x_j$ according to $\PP(V_j\!=\!x_j)$
(using the lookup table representation).
Conjoin $(x_1,...,x_n)$ to form a complete configuration.
%\end{itemize}

\subsubsection*{2. Evaluation}

%\begin{itemize}
%\item[{\bf I1}.] {\em Evaluate}.
Given a complete configuration $\xvec=(x_1,...,x_n)$,
look up the probability of each component $x_j$ and then multiply
the answers together.
\[
\PP(V_1\!=\!x_1,...,V_n\!=\!x_n)\;\;=\;\;p_{1,x_1}\times\cdots\times p_{n,x_n}
\]
%\end{itemize}

\subsubsection*{3. Inference}

\begin{Itemize}
\item[3.a] {\em Marginalization}.
The probability of a partial configuration $(x_1, \ldots, x_k)$ is 
$P(x_1, \ldots, x_k) = P(x_1)\cdot\ldots\cdot P(x_k) =
p_{1,x_1}\cdot\cdots\cdot p_{k,x_k}$, which can be derived strictly
from our assumption in the following way:

%
\begin{eqnarray*}
\lefteqn{\PP(V_1\!=\!x_1,...,V_k\!=\!x_k)}
\\
& = &
\sum_{y_{k+1}}\cdots\sum_{y_n}
\PP(V_1\!=\!x_1,...,V_k\!=\!x_k,V_{k+1}\!=\!y_{k+1},...,V_n\!=\!y_n)
\\
& = &
\sum_{y_{k+1}}\cdots\sum_{y_n}
\PP(V_1\!=\!x_1)\cdots\PP(V_k\!=\!x_k)\PP(V_{k+1}\!=\!y_{k+1})\cdots\PP(V_n\!=\!y_n)
\\
& = &
\PP(V_1\!=\!x_1)\cdots\PP(V_k\!=\!x_k)
\left[\sum_{y_{k+1}}\PP(V_{k+1}\!=\!y_{k+1})\left[\sum_{y_{k+2}}\cdots\left[\sum_{y_n}\PP(V_n\!=\!y_n)\right]\right]\right]
\\
& = &
\PP(V_1\!=\!x_1)\cdots\PP(V_k\!=\!x_k)
\left[\sum_{y_{k+1}}\PP(V_{k+1}\!=\!y_{k+1})\right]\cdots\left[\sum_{y_n}\PP(V_n\!=\!y_n)\right]
\\
& = &
\PP(V_1\!=\!x_1)\cdots\PP(V_k\!=\!x_k)
\\
& = &
p_{1,x_1}\times\cdots\times p_{k,x_k}
\end{eqnarray*}
%
Only have to lookup and multiply $k$ numbers.

\item[3.b] {\em Conditioning}.
%
\begin{eqnarray*}
\lefteqn{\PP(V_{k+1}\!=\!y_{k+1},...,V_n\!=\!y_n|V_1\!=\!x_1,...,V_k\!=\!x_k)}
\\
& = & 
\frac{\PP(V_1\!=\!x_1,...,V_k\!=\!x_k,V_{k+1}\!=\!y_{k+1},...,V_n\!=\!y_n)}%
{\PP(V_1\!=\!x_1,\ldots,V_k\!=\!x_k)}
\\
& = &
\frac{\PP(V_1\!=\!x_1)\cdots\PP(V_k\!=\!x_k)\PP(V_{k+1}\!=\!y_{k+1})\cdots\PP(V_n\!=\!y_n)}%
{\PP(V_1\!=\!x_1)\cdots\PP(V_k\!=\!x_k)}
\\
& = &
\PP(V_{k+1}\!=\!y_{k+1})\cdots\PP(V_n\!=\!y_n)
\\
& = &
p_{k+1,y_{k+1}}\times\cdots\times p_{n,y_n}
\end{eqnarray*}
%
Only have to lookup and multiply $n-k$ numbers.

\item[3.c] {\em Completion}.
%
\begin{eqnarray*}
y_{k+1}^*,...,y_n^*
& = & 
\argmax_{y_{k+1},...,y_n}
\PP(V_{k+1}\!=\!y_{k+1},...,V_n\!=\!y_n|V_1\!=\!x_1,...,V_k\!=\!x_k)
\\
& = &
\argmax_{y_{k+1},...,y_n}
\PP(V_{k+1}\!=\!y_{k+1})\cdots\PP(V_n\!=\!y_n)
\\
& = &
\argmax_{y_{k+1}}\PP(V_{k+1}\!=\!y_{k+1})\left[\argmax_{y_{k+2}}\cdots\left[\argmax_{y_n}\PP(V_n\!=\!y_n)\right]\right]
\\
&&\mbox{(Since $\max$ and $\argmax$ distributes over product just like sum.}\\
&&\mbox{That is, $\max_i a x_i=a\max_i x_i$ (for $a,x_i\geq0$)}\\
&&\mbox{just like $\sum_i a x_i=a\sum_i x_i$.)}\\
& = &
\left[\argmax_{y_{k+1}}\PP(V_{k+1}\!=\!y_{k+1})\right]\cdots\left[\argmax_{y_n}\PP(V_n\!=\!y_n)\right]
\\
& = &
\left[\argmax_{y_{k+1}} p_{k+1,y_{k+1}}\right]\cdots\left[\argmax_{y_n} p_{n,y_n}\right]
\end{eqnarray*}
%
Only have to search through $m$ possible completions for each of the
$n-k$ variables separately.

\end{Itemize}

% Lecture 07
%\lecture{28-Sep-2006}{Na\"{\i}ve Bayes Model}

\subsubsection*{Note}

It is important to note a general rule which we used to separate
summations in the above tasks of Marginalization and Completion:
If $a$ and $b$ are two variables, and $f(a)$ and $g(b)$ are two
functions, such that $f(a)$ does not depend on $b$ and $g(b)$ does
not depend on $a$, then:
\begin{eqnarray*}\label{sumseparation}
\sum_a \sum_b f(a)g(b) &=&  \sum_a f(a) \left(\sum_b g(b)\right)\\
&& \textrm{(because $f(a)$ is a constant for summation over $b$)}\\
&=& \left( \sum_b g(b) \right)\cdot\left(\sum_a f(a) \right)\\
&& \textrm{(because $\sum_b g(b)$ is a constant for sumation over
$a$)}\\
&=& \left( \sum_a f(a) \right)\cdot\left(\sum_b g(b) \right)
\end{eqnarray*}
If we assume that $f(a)\ge 0$ and $g(b)\ge 0$, the same rule applies
for $\argmax_a$ and $\argmax_b$:
\begin{eqnarray*}
\lefteqn{\argmax_a\, \argmax_b f(a)g(b) =}\\
&=&  \argmax_a f(a) \left(\argmax_b g(b)\right)\\
&& \textrm{(because $f(a)$ is a constant for maximization over $b$)}\\
&=& \left( \argmax_b g(b) \right)\cdot\left(\argmax_a f(a) \right)\\
&& \textrm{(because $\argmax_b g(b)$ is a constant for maximization over
$a$)}\\
&=& \left( \argmax_a f(a) \right)\cdot\left(\argmax_b g(b) \right)
\end{eqnarray*}

\subsubsection*{Joint Distribution Model vs. Fully Independent Model}

Although it addresses the previous problems with the joint
distribution model, the fully independent model suffers from two
strong assumptions and two little structure, so it usually does not
reflect accurately the reality.

A compromise between the previous two ``extreme'' models, the joint
distribution model and the fully independent model, are the
\terms{structured probability models.}
Structured probability models are more efficient than the joint
distribution model and they address the issue of sparse data problem,
and in the same time they do model important dependencies, unlike the
fully independent model.

One of the simplest models of this kind is the Na\"{\i}ve Bayes Model.

%slide:{\bf Natural Language Phenomena}
%slide:
%slide:\begin{itemize}
%slide:\item Agreement
%slide:\item Movement
%slide:\item Subcategorization
%slide:\end{itemize}
%slide:\end{document}
